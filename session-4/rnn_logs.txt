20-06-11 11:07:41, INFO: [('test_path', '/content/gdrive/My Drive/MOUNT/dataset/test_encoded.txt'), ('train_path', '/content/gdrive/My Drive/MOUNT/dataset/train_encoded.txt'), ('vocab_path', '/content/gdrive/My Drive/MOUNT/dataset/vocab.txt')]
20-06-11 11:07:42, WARNING: From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
20-06-11 11:07:42, WARNING: From RNN.py:62: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.
20-06-11 11:07:42, WARNING: From RNN.py:73: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API
20-06-11 11:07:42, WARNING: From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:740: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.add_weight` method instead.
20-06-11 11:07:42, WARNING: From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:744: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
20-06-11 11:08:00, WARNING: From RNN.py:127: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

20-06-11 11:10:52, INFO: Step: 20 - loss: 0.8891077637672424
20-06-11 11:11:10, INFO: Step: 40 - loss: 3.8745107650756836
20-06-11 11:11:28, INFO: Step: 60 - loss: 3.517507314682007
20-06-11 11:11:46, INFO: Step: 80 - loss: 1.844083547592163
20-06-11 11:12:04, INFO: Step: 100 - loss: 2.5761759281158447
20-06-11 11:12:23, INFO: Step: 120 - loss: 5.314517974853516
20-06-11 11:12:41, INFO: Step: 140 - loss: 2.678567409515381
20-06-11 11:12:59, INFO: Step: 160 - loss: 3.2982192039489746
20-06-11 11:13:17, INFO: Step: 180 - loss: 5.7052693367004395
20-06-11 11:13:35, INFO: Step: 200 - loss: 3.8324594497680664
20-06-11 11:13:53, INFO: Step: 220 - loss: 3.607114315032959
20-06-11 11:15:57, INFO: Epoch: 1
20-06-11 11:15:58, INFO: Accuracy on test data: 4.517230673703819
20-06-11 11:16:10, INFO: Step: 240 - loss: 2.601996660232544
20-06-11 11:16:28, INFO: Step: 260 - loss: 2.498981237411499
20-06-11 11:16:46, INFO: Step: 280 - loss: 2.5470380783081055
20-06-11 11:17:04, INFO: Step: 300 - loss: 2.411858558654785
20-06-11 11:17:23, INFO: Step: 320 - loss: 2.3805599212646484
20-06-11 11:17:41, INFO: Step: 340 - loss: 2.352893352508545
20-06-11 11:18:00, INFO: Step: 360 - loss: 2.182119607925415
20-06-11 11:18:18, INFO: Step: 380 - loss: 2.2290308475494385
20-06-11 11:18:37, INFO: Step: 400 - loss: 2.0360240936279297
20-06-11 11:18:55, INFO: Step: 420 - loss: 1.9311081171035767
20-06-11 11:19:13, INFO: Step: 440 - loss: 1.7944029569625854
20-06-11 11:21:21, INFO: Epoch: 2
20-06-11 11:21:23, INFO: Accuracy on test data: 67.55665942253958
20-06-11 11:21:29, INFO: Step: 460 - loss: 1.4237103462219238
20-06-11 11:21:47, INFO: Step: 480 - loss: 1.121639609336853
20-06-11 11:22:05, INFO: Step: 500 - loss: 0.9799211025238037
20-06-11 11:22:23, INFO: Step: 520 - loss: 1.0157957077026367
20-06-11 11:22:42, INFO: Step: 540 - loss: 1.105129599571228
20-06-11 11:23:00, INFO: Step: 560 - loss: 0.9810882806777954
20-06-11 11:23:18, INFO: Step: 580 - loss: 0.8060122728347778
20-06-11 11:23:36, INFO: Step: 600 - loss: 0.7015289068222046
20-06-11 11:23:55, INFO: Step: 620 - loss: 0.508054792881012
20-06-11 11:24:13, INFO: Step: 640 - loss: 0.864637017250061
20-06-11 11:24:31, INFO: Step: 660 - loss: 0.6645128726959229
20-06-11 11:26:47, INFO: Epoch: 3
20-06-11 11:26:49, INFO: Accuracy on test data: 82.92455759081031
20-06-11 11:26:49, INFO: Step: 680 - loss: 0.331666499376297
20-06-11 11:27:07, INFO: Step: 700 - loss: 0.3765380084514618
20-06-11 11:27:25, INFO: Step: 720 - loss: 0.2749670147895813
20-06-11 11:27:44, INFO: Step: 740 - loss: 0.2852797508239746
20-06-11 11:28:03, INFO: Step: 760 - loss: 0.2981865108013153
20-06-11 11:28:21, INFO: Step: 780 - loss: 0.26141178607940674
20-06-11 11:28:40, INFO: Step: 800 - loss: 0.2648932933807373
20-06-11 11:28:58, INFO: Step: 820 - loss: 0.28411826491355896
20-06-11 11:29:17, INFO: Step: 840 - loss: 0.3731454908847809
20-06-11 11:29:35, INFO: Step: 860 - loss: 0.27004197239875793
20-06-11 11:29:54, INFO: Step: 880 - loss: 0.40448105335235596
20-06-11 11:30:13, INFO: Step: 900 - loss: 0.5008741617202759
20-06-11 11:32:17, INFO: Epoch: 4
20-06-11 11:32:18, INFO: Accuracy on test data: 92.99906861223222
20-06-11 11:32:32, INFO: Step: 920 - loss: 0.1178036481142044
20-06-11 11:32:51, INFO: Step: 940 - loss: 0.12570223212242126
20-06-11 11:33:09, INFO: Step: 960 - loss: 0.09465564787387848
20-06-11 11:33:28, INFO: Step: 980 - loss: 0.06177011877298355
20-06-11 11:33:47, INFO: Step: 1000 - loss: 0.14990957081317902
20-06-11 11:34:06, INFO: Step: 1020 - loss: 0.17328713834285736
20-06-11 11:34:25, INFO: Step: 1040 - loss: 0.10547437518835068
20-06-11 11:34:43, INFO: Step: 1060 - loss: 0.07134230434894562
20-06-11 11:35:02, INFO: Step: 1080 - loss: 0.13157528638839722
20-06-11 11:35:21, INFO: Step: 1100 - loss: 0.06733621656894684
20-06-11 11:35:39, INFO: Step: 1120 - loss: 0.05877446010708809
20-06-11 11:37:49, INFO: Epoch: 5
20-06-11 11:37:50, INFO: Accuracy on test data: 96.89537410742005
20-06-11 11:37:58, INFO: Step: 1140 - loss: 0.03470412269234657
20-06-11 11:38:16, INFO: Step: 1160 - loss: 0.021475743502378464
20-06-11 11:38:35, INFO: Step: 1180 - loss: 0.04227212816476822
20-06-11 11:38:54, INFO: Step: 1200 - loss: 0.05631205439567566
20-06-11 11:39:12, INFO: Step: 1220 - loss: 0.027187608182430267
20-06-11 11:39:30, INFO: Step: 1240 - loss: 0.03250298649072647
20-06-11 11:39:49, INFO: Step: 1260 - loss: 0.12151578813791275
20-06-11 11:40:07, INFO: Step: 1280 - loss: 0.020975053310394287
20-06-11 11:40:26, INFO: Step: 1300 - loss: 0.0210212804377079
20-06-11 11:40:44, INFO: Step: 1320 - loss: 0.07335010170936584
20-06-11 11:41:03, INFO: Step: 1340 - loss: 0.07285203039646149
20-06-11 11:43:17, INFO: Epoch: 6
20-06-11 11:43:19, INFO: Accuracy on test data: 98.68053399565352
20-06-11 11:43:21, INFO: Step: 1360 - loss: 0.028704414144158363
20-06-11 11:43:39, INFO: Step: 1380 - loss: 0.009248444810509682
20-06-11 11:43:58, INFO: Step: 1400 - loss: 0.012581612914800644
20-06-11 11:44:17, INFO: Step: 1420 - loss: 0.06070081889629364
20-06-11 11:44:35, INFO: Step: 1440 - loss: 0.009296449832618237
20-06-11 11:44:54, INFO: Step: 1460 - loss: 0.029646363109350204
20-06-11 11:45:12, INFO: Step: 1480 - loss: 0.011706709861755371
20-06-11 11:45:31, INFO: Step: 1500 - loss: 0.05895528942346573
20-06-11 11:45:49, INFO: Step: 1520 - loss: 0.01096403133124113
20-06-11 11:46:08, INFO: Step: 1540 - loss: 0.02366718091070652
20-06-11 11:46:26, INFO: Step: 1560 - loss: 0.070913165807724
20-06-11 11:46:44, INFO: Step: 1580 - loss: 0.01930934190750122
20-06-11 11:48:45, INFO: Epoch: 7
20-06-11 11:48:46, INFO: Accuracy on test data: 99.03756597330022
20-06-11 11:49:02, INFO: Step: 1600 - loss: 0.00732912914827466
20-06-11 11:49:21, INFO: Step: 1620 - loss: 0.012713403441011906
20-06-11 11:49:39, INFO: Step: 1640 - loss: 0.014472942799329758
20-06-11 11:49:58, INFO: Step: 1660 - loss: 0.07209967821836472
20-06-11 11:50:17, INFO: Step: 1680 - loss: 0.01230916939675808
20-06-11 11:50:35, INFO: Step: 1700 - loss: 0.034229908138513565
20-06-11 11:50:54, INFO: Step: 1720 - loss: 0.012347858399152756
20-06-11 11:51:12, INFO: Step: 1740 - loss: 0.013361156918108463
20-06-11 11:51:31, INFO: Step: 1760 - loss: 0.0071917031891644
20-06-11 11:51:50, INFO: Step: 1780 - loss: 0.004169936757534742
20-06-11 11:52:08, INFO: Step: 1800 - loss: 0.0045523312874138355
20-06-11 11:54:14, INFO: Epoch: 8
20-06-11 11:54:16, INFO: Accuracy on test data: 99.25488978578082
20-06-11 11:54:25, INFO: Step: 1820 - loss: 0.004856827203184366
20-06-11 11:54:44, INFO: Step: 1840 - loss: 0.0034251369070261717
20-06-11 11:55:02, INFO: Step: 1860 - loss: 0.009258560836315155
20-06-11 11:55:21, INFO: Step: 1880 - loss: 0.004445248283445835
20-06-11 11:55:39, INFO: Step: 1900 - loss: 0.08027653396129608
20-06-11 11:55:58, INFO: Step: 1920 - loss: 0.004791477229446173
20-06-11 11:56:16, INFO: Step: 1940 - loss: 0.012211969122290611
20-06-11 11:56:35, INFO: Step: 1960 - loss: 0.0038950559683144093
20-06-11 11:56:54, INFO: Step: 1980 - loss: 0.004000409040600061
20-06-11 11:57:12, INFO: Step: 2000 - loss: 0.003770359791815281
20-06-11 11:57:14, INFO: Completed
